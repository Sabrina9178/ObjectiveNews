# -*- coding: utf-8 -*-
"""SETN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cgyrv0VK71skYnQvhli7tEWiIdk4a6nf
"""

import datetime
from selenium.webdriver.edge.options import Options
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv
from tqdm import tqdm  # 加入進度條模組

import requests
from fake_useragent import UserAgent  # 使用 fake_useragent 生成隨機標頭
import random
from bs4 import BeautifulSoup

def get_news_content(url):
    # 隨機生成標頭
    user_agent = UserAgent()
    headers = {'User-Agent': user_agent.random}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # 如果請求不成功，則拋出異常
        
        # soup = BeautifulSoup(response.content, 'html.parser')
        # content_elements = soup.find_all('p')
        # news_content = "\n".join([p.text for p in content_elements])

        
        # return news_content

        soup = BeautifulSoup(response.content, 'html.parser')
        fig_elements = soup.find_all('p',style="text-align: center;")
        for fig_element in fig_elements:
            fig_element.decompose()

        fig_elements = soup.find_all('p',style="text-align:center")
        for fig_element in fig_elements:
            fig_element.decompose()
        
        content_div = soup.find('div', id='Content1')  # Find the div with id "Content1"
        #print(content_div)
        if content_div:
            content_elements = content_div.find_all('p')  # Find all <p> elements inside the div
            news_content = "\n".join([p.text for p in content_elements])
            return news_content
        else:
            content_div = soup.find('article')  # Find the div with id "Content1"
            #print(content_div)
            if content_div:
                content_elements = content_div.find_all('p')  # Find all <p> elements inside the div
                news_content = "\n".join([p.text for p in content_elements])
                return news_content
            else:
                return ""
            
    except Exception as e:
        print(f"Failed to fetch news content: {e}")
        return ""


def find_SETN_news(n_hours_ago,driver):

    page = 1
    # 新聞網站 URL
    url = "https://www.setn.com/ViewAll.aspx"
    driver.get(url)

    # 初始化一個空字典來存放時間
    news = {}
    index = 1

    # 初始化已抓取的新聞數量
    num_news_before_scroll = 0
    
    print("SETN：")
    # 持續滾動頁面,直到找不到符合時間範圍的新聞為止
    while True:
        
        # 找到所有新聞項目
        news_items = driver.find_elements(By.CSS_SELECTOR, "div.col-sm-12.newsItems > div")

        # 如果沒有新的新聞項目,退出循環
        if not news_items:
            break

        # 標記本次循環是否找到了符合時間範圍的新聞
        found_new_news = False

        for i in range(num_news_before_scroll, len(news_items)):
            item = news_items[i]
            # 獲取標題
            title_element = item.find_element(By.CSS_SELECTOR, "h3.view-li-title > a")
            news_title = title_element.text if title_element else ""


            # 獲取時間
            time_element = item.find_element(By.CSS_SELECTOR, "time")
            news_time_str = time_element.text if time_element else ""
            news_time = datetime.datetime.strptime(news_time_str, "%m/%d %H:%M")
            news_time = news_time.replace(year=2024)

            # 如果新聞時間在n小時範圍內
            if n_hours_ago <= news_time:
                found_new_news = True


                news_link = title_element.get_attribute('href') if title_element else ""
                category_element = item.find_element(By.CSS_SELECTOR, "div.newslabel-tab > a")
                news_category = category_element.text if category_element else ""

                # print(f"標題: {news_title}")
                # print(f"連結: {news_link}")
                # print(f"時間: {news_time_str}")
                # print(f"分類: {news_category}")
                # print("-------------------")

                news[index] = [news_title,
                               news_link,
                               news_time_str,
                               news_category]
                index += 1
            else:
                break

        num_news_before_scroll = len(news_items)

        # 如果本次循環中沒有找到符合時間範圍的新聞,就退出
        if not found_new_news:
            print(f"\n  stop date-time: {news_time_str}")
            break
        
        print(f"\r  found {len(news)} news, last date-time： {news[len(news)][2]}", end='')
        
        # 模擬滾動到頁面底部
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        driver.execute_script("window.scrollBy(0, -1);")
        time.sleep(1)  # 等待頁面響應

    # # 訪問每個新聞詳情頁,爬取內文
    # for index in tqdm(news, desc='Processing SETN News'):
    #     driver.get(news[index][1])
    #     try:
    #         # 等待页面元素加载完成
    #         WebDriverWait(driver, 300).until(EC.presence_of_element_located((By.CSS_SELECTOR, "p")))
    #         #print("start crawlering", end = '\r')
    #         content_elements = driver.find_elements(By.CSS_SELECTOR, "p")
    #         news_content = "\n".join([p.text for p in content_elements])
            
    #         # print(f"標題: {news[index][0]}")
    #         # print(f"內文: {news_content}")
    #     except:
    #         news_content = ""
    #     news[index].append(news_content)
    print("\n")


    return news
